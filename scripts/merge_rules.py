#!/usr/bin/env python3
"""
AIè§„åˆ™åˆå¹¶è„šæœ¬
âš ï¸ éœ€è¦ä¿®æ”¹çš„åœ°æ–¹å·²ç”¨ ### æ ‡æ³¨ ###
"""

import requests
import re
from pathlib import Path
from datetime import datetime

class AIRulesMerger:
    def __init__(self):
        # ğŸ”§ è¿™é‡Œå¯ä»¥ä¿®æ”¹è§„åˆ™æ¥æºï¼Œæ·»åŠ æˆ–åˆ é™¤
        self.sources = {
            'win-update': 'https://raw.githubusercontent.com/ForestL18/rules-dat/main/ai.txt',
            'category-games-cn': 'https://raw.githubusercontent.com/MetaCubeX/meta-rules-dat/meta/geo/geosite/openai.txt',
            'steamcn': 'https://ruleset.skk.moe/List/domainset/ai.conf',
            'nvidia': 'https://raw.githubusercontent.com/DustinWin/ruleset_geodata/main/rules/ai.txt'
            'cn_site': 'https://raw.githubusercontent.com/ForestL18/rules-dat/main/ai.txt',
            'chinadomains': 'https://raw.githubusercontent.com/MetaCubeX/meta-rules-dat/meta/geo/geosite/openai.txt',
            ### å¦‚æœæƒ³æ·»åŠ æ–°çš„è§„åˆ™æºï¼ŒæŒ‰ç…§è¿™ä¸ªæ ¼å¼æ·»åŠ ï¼š
            ### 'SourceName': 'https://è§„åˆ™åœ°å€.txt',
        }
        
        self.all_domains = set()
        self.all_domain_suffixes = set()
        self.all_domain_keywords = set()
        
    def download_rules(self, url, source_name):
        """ä¸‹è½½è§„åˆ™æ–‡ä»¶"""
        try:
            print(f"æ­£åœ¨ä¸‹è½½ {source_name} è§„åˆ™...")
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"ä¸‹è½½ {source_name} å¤±è´¥: {e}")
            return ""
    
    def parse_rules(self, content, source_name):
        """è§£æè§„åˆ™å†…å®¹"""
        lines = content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
            if not line or line.startswith('#') or line.startswith('//'):
                continue
            
            # å¤„ç†ä¸åŒæ ¼å¼çš„è§„åˆ™
            if line.startswith('DOMAIN-SUFFIX,'):
                domain = line.split(',', 1)[1].strip()
                self.all_domain_suffixes.add(domain)
            
            elif line.startswith('DOMAIN,'):
                domain = line.split(',', 1)[1].strip()
                self.all_domains.add(domain)
            
            elif line.startswith('DOMAIN-KEYWORD,'):
                keyword = line.split(',', 1)[1].strip()
                self.all_domain_keywords.add(keyword)
            
            elif line.startswith('.'):
                domain = line.lstrip('.').split(',')[0].strip()
                if domain:
                    self.all_domain_suffixes.add(domain)
            
            elif re.match(r'^[a-zA-Z0-9][-a-zA-Z0-9.]*\.[a-zA-Z]{2,}$', line):
                self.all_domain_suffixes.add(line)
        
        print(f"{source_name}: æ‰¾åˆ°è§„åˆ™ {len(self.all_domains) + len(self.all_domain_suffixes) + len(self.all_domain_keywords)} æ¡")
    
    def generate_text_rules(self, output_file='AIs_merged.txt'):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼è§„åˆ™æ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥å¤´éƒ¨ä¿¡æ¯
            f.write("# AI Services Merged Rules\n")
            f.write("# åˆå¹¶æ¥æº: ForestL18, MetaCubeX, Sukka, DustinWin\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# ç»Ÿè®¡: DOMAIN={len(self.all_domains)}, ")
            f.write(f"DOMAIN-SUFFIX={len(self.all_domain_suffixes)}, ")
            f.write(f"DOMAIN-KEYWORD={len(self.all_domain_keywords)}\n\n")
            
            # å†™å…¥è§„åˆ™
            for domain in sorted(self.all_domains):
                f.write(f"DOMAIN,{domain}\n")
            
            for domain in sorted(self.all_domain_suffixes):
                f.write(f"DOMAIN-SUFFIX,{domain}\n")
            
            for keyword in sorted(self.all_domain_keywords):
                f.write(f"DOMAIN-KEYWORD,{keyword}\n")
        
        print(f"\nâœ… æ–‡æœ¬è§„åˆ™å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def run(self):
        """æ‰§è¡Œåˆå¹¶æµç¨‹"""
        print("=" * 50)
        print("å¼€å§‹åˆå¹¶ AI è§„åˆ™...")
        print("=" * 50)
        
        # ä¸‹è½½å¹¶è§£ææ‰€æœ‰è§„åˆ™
        for source_name, url in self.sources.items():
            content = self.download_rules(url, source_name)
            if content:
                self.parse_rules(content, source_name)
        
        # ç»Ÿè®¡ç»“æœ
        total = len(self.all_domains) + len(self.all_domain_suffixes) + len(self.all_domain_keywords)
        print("\n" + "=" * 50)
        print("åˆå¹¶å®Œæˆï¼")
        print(f"å®Œæ•´åŸŸå: {len(self.all_domains)} æ¡")
        print(f"åŸŸååç¼€: {len(self.all_domain_suffixes)} æ¡")
        print(f"åŸŸåå…³é”®è¯: {len(self.all_domain_keywords)} æ¡")
        print(f"æ€»è§„åˆ™æ•°: {total} æ¡")
        print("=" * 50)
        
        # ç”Ÿæˆæ–‡æœ¬è§„åˆ™
        text_file = self.generate_text_rules()
        return text_file


if __name__ == "__main__":
    merger = AIRulesMerger()
    merger.run()
